{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aae81d28-f36a-46e8-81f6-a32e10eb5b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import modelscope\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "import json\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ecbf8d8-9738-4b53-a27f-a736a2f62a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./muice-dataset-train.catgirl (2).json','r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c1178bb-774d-404c-97d3-bbbc44d8a05b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 0\n",
    "for d in data:\n",
    "    max_len = max(len(d['instruction']),max_len)\n",
    "    max_len = max(len(d['output']),max_len)\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ee76ace-13c9-445d-a47d-d670f091a1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e56dce75-1f23-4c5f-a870-609d6a2cdd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"./DeepSeek\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./DeepSeek\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35cfa8f8-a1ba-43cf-bc8a-bc2a57aab1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def token(example,max_length=180):\n",
    "#     instruction = example['instruction']\n",
    "#     output = example['output']\n",
    "#     prompt = [{\"role\":\"user\",\"content\":instruction}]\n",
    "#     prompt_token = tokenizer.apply_chat_template(prompt,tokenize=False)\n",
    "#     q = tokenizer.encode(prompt_token)\n",
    "#     a = tokenizer.encode(output)\n",
    "#     input_ids = q+a\n",
    "#     labels = a\n",
    "#     attention_mask = [-100]*len(q) + [1]*len(a)\n",
    "#     if len(input_ids)<=max_length:\n",
    "#         padding_length = max_length-len(input_ids)\n",
    "#         input_ids = [-100]*padding_length+ input_ids\n",
    "#         labels = [-100]*(padding_length+len(q)) +labels\n",
    "#         attention_mask = [0]*padding_length+attention_mask\n",
    "#     else:\n",
    "#         input_ids = input_ids[:max_length]\n",
    "#         labels = labels[:max_length]\n",
    "#         attention_mask = attention_mask[:max_length]\n",
    "#     return {\n",
    "#         \"input_ids\":input_ids,\n",
    "#         \"labels\":labels,\n",
    "#         \"attention_mask\":attention_mask\n",
    "#     }\n",
    "# def token(example, max_length=180):\n",
    "#     instruction = example['instruction']\n",
    "#     output = example['output']\n",
    "#     prompt = [{\"role\": \"user\", \"content\": example[\"instruction\"]}]\n",
    "#     prompt_token = tokenizer.apply_chat_template(prompt, tokenize=False)\n",
    "#     tokenized = tokenizer(\n",
    "#         prompt_token,\n",
    "#         text_target=example[\"output\"],\n",
    "#         max_length=max_length,\n",
    "#         padding=\"max_length\",\n",
    "#         truncation=True,\n",
    "#         return_tensors=\"pt\",\n",
    "#     )\n",
    "#     input_ids = tokenized[\"input_ids\"][0]\n",
    "#     labels = tokenized[\"labels\"][0]\n",
    "#     attention_mask = tokenized[\"attention_mask\"][0]\n",
    "  \n",
    "    \n",
    "#     if len(input_ids) < max_length:\n",
    "#         pad_len = max_length - len(input_ids)\n",
    "#         input_ids = [tokenizer.pad_token_id] * pad_len + input_ids\n",
    "#         attention_mask = [0] * pad_len + attention_mask\n",
    "#         labels = [-100] * pad_len + labels\n",
    "#     else:\n",
    "#         input_ids = input_ids[:max_length]\n",
    "#         attention_mask = attention_mask[:max_length]\n",
    "#         labels = labels[:max_length]\n",
    "  # prompt = [{\"role\": \"user\", \"content\": instruction}]\n",
    "    # prompt_token = tokenizer.apply_chat_template(prompt, tokenize=False)\n",
    "    \n",
    "    # q = tokenizer.encode(prompt_token, add_special_tokens=False)\n",
    "    # a = tokenizer.encode(output, add_special_tokens=False)\n",
    "    \n",
    "    # input_ids = q + a\n",
    "    # attention_mask = [-100] * len(q) + [1] * len(a)\n",
    "    # labels = [-100] * len(q) + a\n",
    "def token(example, max_length=180):\n",
    "    instruction = example['instruction']\n",
    "    output = example['output']\n",
    "    prompt = [{\"role\": \"user\", \"content\": example[\"instruction\"]}]\n",
    "    q = tokenizer.apply_chat_template(prompt, tokenize=False,add_generation_prompt=True)\n",
    "    \n",
    "    q_input_ids = tokenizer.encode(q)\n",
    "    a_input_ids = tokenizer.encode(output)\n",
    "    input_ids = q_input_ids+a_input_ids\n",
    "    attention_mask = [1]*len(input_ids)\n",
    "    labels = [-100]*len(q_input_ids)+a_input_ids\n",
    "    if len(input_ids) < max_length:\n",
    "        pad_len = max_length - len(input_ids)\n",
    "        input_ids = [tokenizer.pad_token_id] * pad_len + input_ids\n",
    "        attention_mask = [0] * pad_len + attention_mask\n",
    "        labels = [-100] * pad_len + labels\n",
    "    else:\n",
    "        input_ids = input_ids[:max_length]\n",
    "        attention_mask = attention_mask[:max_length]\n",
    "        labels = labels[:max_length]\n",
    "    \n",
    "    # Sanity check\n",
    "    assert len(input_ids) == max_length\n",
    "    assert len(labels) == max_length\n",
    "    assert len(attention_mask) == max_length\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }\n",
    "\n",
    "data_ids = []\n",
    "for d in data:\n",
    "    id_dict = token(d)\n",
    "    data_ids.append(id_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dcad623-77e7-4592-8385-eacb9977c1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class MyData(Dataset):\n",
    "    def __init__(self,data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(data[\"input_ids\"], dtype=torch.long),\n",
    "            \"labels\": torch.tensor(data[\"labels\"], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(data[\"attention_mask\"], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89fcd266-0649-4e6f-8cc7-1c41a5700664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_ids[:5]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d57da50-8dc5-4093-ad0f-f500c81be078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "my_data = MyData(data_ids)\n",
    "train_dataloader = DataLoader(my_data, shuffle=True, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b8a2c4e-38bc-4bcc-8fc9-98d5a941d097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151643"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1b26319-8b28-40fd-b196-e342159d10ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/483 [00:45<6:03:02, 45.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.780388832092285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/483 [01:41<6:52:40, 51.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.4830708503723145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/483 [02:08<5:24:55, 40.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.272593975067139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/483 [02:34<4:37:45, 34.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.448260307312012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5/483 [03:08<4:34:33, 34.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.095908164978027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/483 [03:37<4:19:31, 32.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.342092990875244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 7/483 [04:04<4:04:06, 30.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.298829555511475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 8/483 [04:30<3:51:09, 29.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.975750207901001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 9/483 [04:57<3:45:03, 28.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.745328903198242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 10/483 [05:24<3:42:09, 28.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.899470090866089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 11/483 [05:58<3:55:43, 29.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.092857837677002\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m---> 20\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     22\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/usr/local/miniconda3/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/miniconda3/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/miniconda3/lib/python3.10/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-6)\n",
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for i,batch in enumerate(train_dataloader):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f42d06a-6667-4fad-9d07-eb0732d81b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# 编码输入\n",
    "input = tokenizer(\"沐雪的功能是什么？\", return_tensors=\"pt\")  # 加上 return_tensors=\"pt\" 以得到 PyTorch 张量格式\n",
    "\n",
    "# 生成回复\n",
    "output_ids = model.generate(**input, max_new_tokens=50)  # 可加参数控制生成长度等\n",
    "\n",
    "# 解码输出\n",
    "response = tokenizer.decode(output_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1997032e-5fb0-4f35-b15e-c41418478a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'沐雪的功能是什么？我之前学过一些关于雪的知识，但对沐雪的了解还不是很透彻。请问沐雪的主要功能是什么？它有什么优点和缺点？我之前学过一些关于雪的知识，但对沐雪的了解还不是很'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75a886ab-fa32-4703-bf16-d3db941f9587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./my_model/tokenizer_config.json',\n",
       " './my_model/special_tokens_map.json',\n",
       " './my_model/tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存模型和 tokenizer\n",
    "save_path = \"./my_model\"\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "408b8d9c-cf8b-4f69-8a10-2380fe6360f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 19.06it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# 读取预训练的模型和tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(save_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad6cf461-5db8-47cf-bff2-b69d28fb8a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 1536)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5361223a-0974-4e5d-9d57-122ceab1b594",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
